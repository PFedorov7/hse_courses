{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modelling (Day 7): Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.cluster import KMeans, MeanShift, DBSCAN, AgglomerativeClustering, SpectralClustering, AffinityPropagation, Birch, MiniBatchKMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Clustering or cluster analysis is an **unsupervised learning problem**. It is often used as a data analysis technique for discovering interesting patterns in data. For example, you may need it for classifying diseases in medical science and classifying customers based on their behavior.\n",
    "\n",
    "**In theory**, data points that are in the **same group should have similar properties** and/or features, while data points in **different groups should have highly dissimilar properties** and/or features. \n",
    "\n",
    "Evaluation of identified clusters is subjective and may require a domain expert, although many clustering-specific quantitative measures do exist. Typically, clustering algorithms are compared academically on synthetic datasets with pre-defined clusters, which an algorithm is expected to discover.\n",
    "\n",
    "\n",
    "Most popular clustering algorithms are\n",
    "- k-means\n",
    "- mean-shift clustering\n",
    "- DBSCAN\n",
    "- EM using GMM\n",
    "- Hierarchical clustering\n",
    "- Spectral clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-Means\n",
    "Conventional k-means requires only a few steps:\n",
    "\n",
    "1. The first step is to randomly select k centroids, where k is equal to the number of clusters you choose. Centroids are data points representing the center of a cluster.\n",
    "\n",
    "2. Assignment of each data point to its nearest centroid\n",
    "\n",
    "3. Recalculation of centroids (e.g. the mean of all the points for each cluster)\n",
    "\n",
    "4. Repeat steps 2-3 untill the centroid positions do not change\n",
    "\n",
    "The quality of the cluster assignments is determined by computing the sum of the squared error (SSE).\n",
    "\n",
    "**NOTE:** Multiple runs of k-means may provide different clusers. See the example of 2 different runs:\n",
    "\n",
    "![kmeansURL](https://files.realpython.com/media/centroids_iterations.247379590275.gif \"k-means\")\n",
    "\n",
    "To solve the problem , researchers commonly run several initializations of the entire k-means algorithm and choose the cluster assignments from the initialization with the lowest SSE.\n",
    "\n",
    "Thankfully, there’s **a robust implementation** of k-means clustering in Python from the popular machine learning package **`scikit-learn`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of the random data\n",
    "\n",
    "You can generate the data using `make_blobs()`, a convenience function in **scikit-learn** used to generate synthetic clusters. `make_blobs()` uses these parameters:\n",
    "- **n_samples** is the total number of samples to generate.\n",
    "- **centers** is the number of centers to generate.\n",
    "- **cluster_std** is the standard deviation.\n",
    "\n",
    "The functions returns a tuple of two values: the data and cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data.\n",
    "features, true_labels = make_blobs(n_samples=200, centers=3, cluster_std=2.75, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`random_states` (datatype: int) determines random number generation for dataset creation. Here the parameter is set to an integer value so you can follow the data presented in the tutorial. In practice, it’s best to leave `random_state` as the default value, `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Standartization\n",
    "\n",
    "Data sets usually contain numerical features that have been measured in different units, such as height (in cm) and weight (in kg). A machine learning algorithm would consider weight less important than height only because the values for weight are smaller and have lower variability from person to person.\n",
    "\n",
    "Clustering algorithms need to consider all features on an even playing field. That means **the values for all features must be transformed to the same scale**. The process of transforming numerical features to use the same scale is known as feature scaling. It’s an important data preprocessing step for clustering algorithms because it can have a significant impact on the performance of your algorithm. There are several approaches to implementing feature scaling. More information is provided [here](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
    "\n",
    "In this example, you’ll use the `StandardScaler` class. This class implements a type of feature scaling called **standardization**. Standardization scales the values for each numerical feature in your dataset so that the features have a mean of 0 and standard deviation of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means has the following input parameters:\n",
    "\n",
    "- **init controls** the initialization technique. The standard version of the k-means algorithm is implemented by setting init to \"random\". Setting this to \"k-means++\" employs an advanced trick to speed up convergence, which you’ll use later.\n",
    "\n",
    "- **n_clusters** sets the number of clusters $default=8$.\n",
    "\n",
    "- **n_init** sets the number of initializations to perform. This is important because two runs can converge on different cluster assignments $default=10$.\n",
    "\n",
    "- **max_iter** sets the number of maximum iterations for each initialization of the k-means algorithm $default=300$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(init=\"random\",n_clusters=3,n_init=10,max_iter=300,random_state=42)\n",
    "kmeans.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics from the initialization run with the lowest SSE are available as attributes of kmeans after calling `.fit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lowest SSE value\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final locations of the centroid\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of iterations required to converge\n",
    "kmeans.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster assignments are stored as a one-dimensional NumPy array in `kmeans.labels_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results: Initial Data with Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.gca()\n",
    "plt.scatter(features[:,0], features[:,1], c=true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results: K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.gca()\n",
    "plt.scatter(features[:,0], features[:,1], c=kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the order of the cluster labels for the first two data objects was flipped. The order was [1, 0] in true_labels but [0, 1] in kmeans.labels_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Appropriate Number of Clusters\n",
    "One way to evaluate the appropriate number of clusters is to use the following methods:\n",
    "1. **The elbow method**\n",
    "2. **The silhouette coefficient**\n",
    "\n",
    "These are often used as **complementary evaluation techniques** rather than one being preferred over the other. To perform the elbow method, run several k-means, increment k with each iteration, and record the SSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, init=\"random\", n_init=10,max_iter=300,random_state=42)\n",
    "    kmeans.fit(scaled_features)\n",
    "    sse.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you plot SSE as a function of the number of clusters, notice that SSE continues to decrease as you increase k. As more centroids are added, the distance from each point to its closest centroid will decrease.\n",
    "\n",
    "There’s a sweet spot where the SSE curve starts to bend known as **the elbow point**. The x-value of this point is thought to be a reasonable trade-off between error and number of clusters. In this example, the elbow is located at x=3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 11), sse)\n",
    "plt.xticks(range(1, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the elbow point in the SSE curve isn’t always straightforward. If you’re having trouble choosing the elbow point of the curve, then you could use a Python package, `kneed`, to identify the elbow point programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = KneeLocator(range(1, 11), sse, curve=\"convex\", direction=\"decreasing\")\n",
    "kl.elbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **silhouette coefficient** is a measure of cluster cohesion and separation. It quantifies how well a data point fits into its assigned cluster based on two factors:\n",
    "\n",
    "- How close the data point is to other points in the cluster\n",
    "- How far away the data point is from points in other clusters\n",
    "\n",
    "Silhouette coefficient values range between -1 and 1. Larger numbers indicate that samples are closer to their clusters than they are to other clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `silhouette score()` function, which is implemented in `scikit-learn`, needs a minimum of two clusters, or it will raise an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list holds the silhouette coefficients for each k\n",
    "silhouette_coefficients = []\n",
    "\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, init=\"random\", n_init=10,max_iter=300,random_state=42)\n",
    "    kmeans.fit(scaled_features)\n",
    "    score = silhouette_score(scaled_features, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 11), silhouette_coefficients)\n",
    "plt.xticks(range(2, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mean Shift\n",
    "**Mean shift clustering** involves finding and adapting centroids based on the density of examples in the feature space. A centroid-based algorithm meaning that the goal is to locate the center points of each group/class, which works by updating candidates for center points to be the mean of the points within the sliding-window.\n",
    "\n",
    "Main steps:\n",
    "\n",
    "1. Begin with a circular sliding window having its center at a randomly selected point C with radius r as the kernel. \n",
    "2. At every iteration, the window shifts towards the denser regions by changing the center point to the mean of the points within the window.\n",
    "3. Continue shifting the window according to the mean until you reach the point where you accommodate the maximum number of points within it.\n",
    "4. Repeat this process with multiple sliding windows until you come to a situation wherein all the points will lie within a window. In the case of overlapping of windows, the window having a higher number of points will prevail.\n",
    "\n",
    "The main problem of the algorithm is to select the window size (the bandwidth). If not given, the bandwidth is estimated automatically using `sklearn.cluster.estimate_bandwidth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mshift = MeanShift()\n",
    "mshift.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the results: Mean Shift clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.gca()\n",
    "plt.scatter(features[:,0], features[:,1], c=mshift.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DBSCAN\n",
    "\n",
    "DBSCAN is a Density-Based Spatial Clustering of Applications with Noise. This algorithm does not require a pre-set number of clusters and allow to identify outliers as noise. It also finds arbitrarily shaped and sized clusters quite well.\n",
    "\n",
    "\n",
    "Main steps:\n",
    "1. Start with a random unvisited data point. All points within a distance eps(ɛ) are the neighborhood points. \n",
    "2. If # of neighborhood points > minPoints, the current data point becomes the first point in the cluster. Otherwise, the point gets labeled as ‘Noise.’ \n",
    "3. All points within the distance ɛ become part of the same cluster. \n",
    "4. Continue with the process until you visit and label each point within the ɛ neighborhood of the cluster. \n",
    "5. Repeat the procedure for all the new points added to the cluster group.\n",
    "6. On completion of the process, start again with a new unvisited point thereby leading to the discovery of more clusters or noise. At the end of the process, you ensure that you mark each point as either cluster or noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.3)\n",
    "dbscan.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the results: DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.gca()\n",
    "plt.scatter(features[:,0], features[:,1], c=dbscan.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EM using GMM\n",
    "\n",
    "EM using GMM is an Expectation-Maximization (EM) clustering using Gaussian Mixture Models (GMM).\n",
    "\n",
    "**Main assumbption**: Every single cluster has a Gaussian distribution.\n",
    "\n",
    "**Goal**: to find out the parameters of the Gaussian for each cluster\n",
    "\n",
    "Main steps:\n",
    "\n",
    "1. Define the total number of clusters and randomly initialize the Gaussian distribution parameters for each one of them.\n",
    "2. With this background, calculate the probability of each data point belonging to a particular cluster. The closer the point is to the Gaussian’s center, the better are the chances of it belonging to the cluster.\n",
    "3. Determine a new set of parameters for the Gaussian distributions to maximize the probabilities of data points within the clusters. A weighted sum of data point positions is used to compute these probabilities. The likelihood of the data point belonging to the particular cluster is the weight factor.\n",
    "4. Repeat the steps 2 and 3 until convergence where there is not much variation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_gmm = GaussianMixture(n_components=3)\n",
    "em_gmm_labels=em_gmm.fit_predict(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the results: EM with GMM clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.gca()\n",
    "plt.scatter(features[:,0], features[:,1], c=em_gmm_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that EM with GMM also provides information about the probabilities for a particular point to be assigned in rach cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = em_gmm.predict_proba(scaled_features)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see are the points which are on the edge of both clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6\n",
    "for i in range(len(probabilities)):\n",
    "    if max(probabilities[i])<0.6:\n",
    "        print(features[i], probabilities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agglomerative Clustering\n",
    "\n",
    "Agglomerative clustering involves merging examples until the desired number of clusters is achieved. It is implemented via the `AgglomerativeClustering` class and the main configuration to tune is the “n_clusters” set, an estimate of the number of clusters in the data, e.g. 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative = AgglomerativeClustering(n_clusters=3)\n",
    "agglomerative.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you can construct a dendrogram to choose an appropriate number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "dend = sch.dendrogram(sch.linkage(scaled_features, method='ward'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the results: Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.gca()\n",
    "plt.scatter(features[:,0], features[:,1], c=agglomerative.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spectral Clustering\n",
    "\n",
    "Spectral Clustering is a general class of clustering methods, drawn from linear algebra. It uses the top eigenvectors of a matrix derived from the distance between points.\n",
    "\n",
    "It is implemented via the `SpectralClustering` class and the main Spectral Clustering is a general class of clustering methods, drawn from linear algebra. to tune is the “n_clusters” hyperparameter used to specify the estimated number of clusters in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral = SpectralClustering(n_clusters=3)\n",
    "spectral.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the results: Spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.gca()\n",
    "plt.scatter(features[:,0], features[:,1], c=spectral.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare clustering methods using the silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the silhouette scores for each algorithm\n",
    "kmeans_silhouette = silhouette_score(scaled_features, kmeans.labels_).round(2)\n",
    "mshift_silhouette = silhouette_score(scaled_features, mshift.labels_).round(2)\n",
    "dbscan_silhouette = silhouette_score(scaled_features, dbscan.labels_).round(2)\n",
    "em_gmm_silhouette = silhouette_score(scaled_features, em_gmm_labels).round(2)\n",
    "agglomerative_silhouette = silhouette_score(scaled_features, agglomerative.labels_).round(2)\n",
    "spectral_silhouette = silhouette_score(scaled_features, spectral.labels_).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the silhouette coefficient for each of the algorithms and compare them. A higher silhouette coefficient suggests better clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mshift_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_gmm_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that k-means, agglomerative and spectral clustering algorithms perform better on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Overal Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 3, figsize=(15, 10), sharex=True, sharey=True, dpi=600)\n",
    "fig.suptitle(f\"Clustering Algorithm Comparison\", fontsize=16)\n",
    "#fte_colors = {0: \"#008fd5\", 1: \"#fc4f30\", 2: \"#1f77b4\", 3: \"#ff7f0e\",4: \"#2ca02c\",5: \"#d62728\", -1: \"#9467bd\"}\n",
    "\n",
    "# The Ground Truth plot\n",
    "ax[0,0].scatter(features[:, 0], features[:, 1], c=true_labels)\n",
    "ax[0,0].set_title(f\"Ground Truth\", fontdict={\"fontsize\": 12})\n",
    "\n",
    "# The k-means plot\n",
    "ax[0,1].scatter(features[:, 0], features[:, 1], c=kmeans.labels_)\n",
    "ax[0,1].set_title(f\"K-means\\nSilhouette: {kmeans_silhouette}\", fontdict={\"fontsize\": 12})\n",
    "\n",
    "# The mean-shift plot\n",
    "ax[0,2].scatter(features[:, 0], features[:, 1], c=mshift.labels_)\n",
    "ax[0,2].set_title(f\"Mean-shift\\nSilhouette: {dbscan_silhouette}\", fontdict={\"fontsize\": 12})\n",
    "\n",
    "# The dbscan plot\n",
    "ax[1,0].scatter(features[:, 0], features[:, 1], c=dbscan.labels_)\n",
    "ax[1,0].set_title(f\"DBSCAN\\nSilhouette: {dbscan_silhouette}\", fontdict={\"fontsize\": 12})\n",
    "\n",
    "# The EM using GMM\n",
    "ax[1,1].scatter(features[:, 0], features[:, 1], c=em_gmm_labels)\n",
    "ax[1,1].set_title(f\"EM using GMM\\nSilhouette: {em_gmm_silhouette}\", fontdict={\"fontsize\": 12})\n",
    "\n",
    "# The agglomerative plot\n",
    "ax[1,2].scatter(features[:, 0], features[:, 1], c=agglomerative.labels_)\n",
    "ax[1,2].set_title(f\"Agglomerative clustering\\nSilhouette: {agglomerative_silhouette}\", fontdict={\"fontsize\": 12})\n",
    "\n",
    "# The agglomerative plot\n",
    "ax[2,1].scatter(features[:, 0], features[:, 1], c=spectral.labels_)\n",
    "ax[2,1].set_title(f\"Spectral clustering\\nSilhouette: {spectral_silhouette}\", fontdict={\"fontsize\": 12})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other clustering methods\n",
    "**Affinity Propagation**: involves finding a set of exemplars that best summarize the data. The algorithmic complexity of affinity propagation is quadratic in the number of points.\n",
    "\n",
    "Damping factor (between 0.5 and 1) is the extent to which the current value is maintained relative to incoming values (weighted 1 - damping). This in order to avoid numerical oscillations when updating these values (messages). **The default value is 0.5**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity = AffinityPropagation(damping=0.9)\n",
    "affinity.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the results: Affinity Propagation clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.gca()\n",
    "plt.scatter(features[:,0], features[:,1], c=affinity.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BIRCH Clustering** (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n",
    "\n",
    "It is implemented via the Birch class and the main configuration to tune is the “threshold” and “n_clusters” hyperparameters, the latter of which provides an estimate of the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch = Birch(threshold=0.01, n_clusters=3)\n",
    "birch.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the results: BIRCH clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.gca()\n",
    "plt.scatter(features[:,0], features[:,1], c=birch.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-Batch K-Means** \n",
    "\n",
    "Mini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = MiniBatchKMeans(n_clusters=3)\n",
    "minibatch.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the results:Mini-Batch K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.gca()\n",
    "plt.scatter(features[:,0], features[:,1], c=minibatch.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HDBSCAN\n",
    "\n",
    "HDBSCAN is a clustering algorithm which extends DBSCAN by converting it into a hierarchical clustering algorithm, and then using a technique to extract a flat clustering based in the stability of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=2, gen_min_span_tree=True)\n",
    "clusterer.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the results: HDBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.gca()\n",
    "plt.scatter(features[:,0], features[:,1], c=clusterer.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the Data\n",
    "\n",
    "This time, use make_moons() to generate synthetic data in the shape of crescents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, true_labels = make_moons(n_samples=250, noise=0.05, random_state=42)\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.gca()\n",
    "plt.scatter(features[:,0], features[:,1], c=true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform clustering algorithms on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "dbscan = DBSCAN(eps=0.3)\n",
    "\n",
    "# Fit the algorithms to the features\n",
    "kmeans.fit(scaled_features)\n",
    "dbscan.fit(scaled_features)\n",
    "\n",
    "# Compute the silhouette scores for each algorithm\n",
    "kmeans_silhouette = silhouette_score(scaled_features, kmeans.labels_).round(2)\n",
    "dbscan_silhouette = silhouette_score(scaled_features, dbscan.labels_).round (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the silhouette coefficient for each of the two algorithms and compare them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette coefficient is higher for the k-means algorithm. Let us vizualize the results of the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 6), sharex=True, sharey=True)\n",
    "fig.suptitle(f\"Clustering Algorithm Comparison: Crescents\", fontsize=16)\n",
    "fte_colors = {0: \"#008fd5\", 1: \"#fc4f30\",}\n",
    "\n",
    "# The k-means plot\n",
    "km_colors = [fte_colors[label] for label in kmeans.labels_]\n",
    "ax1.scatter(scaled_features[:, 0], scaled_features[:, 1], c=km_colors)\n",
    "ax1.set_title(f\"k-means\\nSilhouette: {kmeans_silhouette}\", fontdict={\"fontsize\": 12})\n",
    "\n",
    "# The dbscan plot\n",
    "db_colors = [fte_colors[label] for label in dbscan.labels_]\n",
    "ax2.scatter(scaled_features[:, 0], scaled_features[:, 1], c=db_colors)\n",
    "ax2.set_title(f\"DBSCAN\\nSilhouette: {dbscan_silhouette}\", fontdict={\"fontsize\": 12})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DBSCAN algorithm appears to find more natural clusters according to the shape of the data. This suggests that you need a better method to compare the performance of these two clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Clustering Performance Using the Ground Truth labels\n",
    "The elbow method and silhouette coefficient evaluate clustering performance **without the use of ground truth labels**. \n",
    "\n",
    "**In practice, it’s rare to encounter datasets that have ground truth labels.** However, since the ground truth labels are known for this data, it’s possible to use a clustering metric that considers labels in its evaluation. \n",
    "\n",
    "You can use the `scikit-learn` implementation of a common metric called **the adjusted rand index (ARI)**. Unlike the silhouette coefficient, the ARI uses true cluster assignments to measure the similarity between true and predicted labels. More abour clustering metrics is available [here](https://scikit-learn.org/stable/modules/classes.html#clustering-metrics).\n",
    "\n",
    "Compare the clustering results of DBSCAN and k-means using ARI as the performance metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_kmeans = adjusted_rand_score(true_labels, kmeans.labels_)\n",
    "ari_dbscan = adjusted_rand_score(true_labels, dbscan.labels_)\n",
    "\n",
    "print(round(ari_kmeans, 2))\n",
    "print(round(ari_dbscan, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ARI output values range between -1 and 1. A score close to 0.0 indicates random assignments, and a score close to 1 indicates perfectly labeled clusters. Based on the above output, you can see that the silhouette coefficient was misleading. ARI shows that DBSCAN is the best choice for the synthetic crescents example as compared to k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Image Segmentation (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # If not installed please use \"pip install opencv-python\"\n",
    "import urllib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_image(url):\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to make image segmentation for one of the pictures from the website of [the HSE University](https://www.hse.ru/buildinghse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = url_to_image(r\"https://www.hse.ru/data/2020/10/28/1359150404/3%D0%A8%D0%B0%D0%B1%D0%BE%D0%BB%D0%BE%D0%B2%D0%BA%D0%B0_%D0%94%D0%9F-83.jpg\") \n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to prepare the data for K means. The image is a 3-dimensional shape but to apply k-means clustering on it we need to reshape it to a 2-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the image into a 2D array of pixels and 3 color values (RGB) and converting to float type \n",
    "pixel_vals = np.float32(image.reshape((-1,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform k-means clustering with 2 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(pixel_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The results (k=2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "centers = np.uint8(kmeans.cluster_centers_)\n",
    "segmented_data = centers[labels.flatten()] \n",
    "segmented_image = segmented_data.reshape((image.shape)) # reshape data into the original image dimensions \n",
    "plt.imshow(segmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we perform k-means clustering with 16 clusters, the resulting image will be the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=16)\n",
    "kmeans.fit(pixel_vals)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "centers = np.uint8(kmeans.cluster_centers_)\n",
    "segmented_data = centers[labels.flatten()] \n",
    "segmented_image = segmented_data.reshape((image.shape)) # reshape data into the original image dimensions \n",
    "plt.imshow(segmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take another picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = url_to_image(r\"https://www.hse.ru/data/2020/06/10/1606383402/24889361570_d148a74529_b.jpg\") \n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the image into a 2D array of pixels and 3 color values (RGB) and converting to float type \n",
    "pixel_vals = np.float32(image.reshape((-1,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform k-means clustering for using 4 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(pixel_vals)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "centers = np.uint8(kmeans.cluster_centers_)\n",
    "segmented_data = centers[labels.flatten()] \n",
    "segmented_image = segmented_data.reshape((image.shape)) # reshape data into the original image dimensions \n",
    "plt.imshow(segmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us vizualize only 1 color (cluster #1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data1 = segmented_data.copy()\n",
    "cluster = 1\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i]!=cluster:\n",
    "        segmented_data1[i] = [255, 255, 255]\n",
    "segmented_image1 = segmented_data1.reshape((image.shape)) # reshape data into the original image dimensions \n",
    "plt.imshow(segmented_image1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image 3\n",
    "Finding the countours of the cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = url_to_image(r\"https://programmerbackpack.com/content/images/2020/04/K-Means-Clustering-image-segmentation---original-image.jpg\") \n",
    "pixel_vals = np.float32(image.reshape((-1,3)))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(pixel_vals)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "centers = np.uint8(kmeans.cluster_centers_)\n",
    "segmented_data = centers[labels.flatten()] \n",
    "segmented_image = segmented_data.reshape((image.shape)) # reshape data into the original image dimensions \n",
    "plt.imshow(segmented_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removedCluster = 1\n",
    "\n",
    "#Change the colors of cluster#1 from the original image (black pixels)\n",
    "cannyImage = np.copy(image).reshape((-1, 3))\n",
    "cannyImage[labels.flatten() == removedCluster] = [0, 0, 0]\n",
    "cannyImage = cv.Canny(cannyImage,100,200).reshape(image.shape)\n",
    "\n",
    "#find countours of the picture and draw them in red\n",
    "initialContoursImage = np.copy(cannyImage)\n",
    "imgray = cv.cvtColor(initialContoursImage, cv2.COLOR_BGR2GRAY)\n",
    "_, thresh = cv2.threshold(imgray, 50, 255, 0)\n",
    "contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cv.drawContours(initialContoursImage, contours, -1, (0,0,255), cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Detect the largest color and draw it in the figure\n",
    "cnt = contours[0]\n",
    "largest_area=0\n",
    "index = 0\n",
    "for contour in contours:\n",
    "    if index > 0:\n",
    "        area = cv.contourArea(contour)\n",
    "        if (area>largest_area):\n",
    "            largest_area=area\n",
    "            cnt = contours[index]\n",
    "    index = index + 1\n",
    "\n",
    "biggestContourImage = np.copy(image)\n",
    "cv.drawContours(biggestContourImage, [cnt], -1, (255,0,0), 3)\n",
    "plt.imshow(biggestContourImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Clustering of digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the load_digits (we already used the dataset on Day 3) which contains 1797 samples of hand-written digits (8x8 pixel images). The total number of dimensions is 64.\n",
    "\n",
    "Clustering algorithms can be used to identify objects of the same groups. Here we will attempt to use k-means to try to identify similar digits **without using the original label information**; this might be similar to a first step in extracting meaning from a new dataset about which you don't have any a priori label information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering can be performed as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "clusters = kmeans.fit_predict(digits.data)\n",
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is 10 clusters in 64 dimensions. Notice that the cluster centers themselves are 64-dimensional points, and can themselves be interpreted as the \"typical\" digit within the cluster. Let's see what these cluster centers look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(8, 3))\n",
    "centers = kmeans.cluster_centers_.reshape(10, 8, 8)\n",
    "for axi, center in zip(ax.flat, centers):\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that **even without the labels**, KMeans is able to find clusters whose centers are recognizable digits, with perhaps the exception of 1 and 8.\n",
    "\n",
    "Because k-means knows nothing about the identity of the cluster, the 0–9 labels may be permuted. We can fix this by matching each learned cluster label with the true labels found in them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(digits.target[mask])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(digits.target, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just a simple k-means algorithm, we discovered the correct grouping for 80% of the input digits! Let's check the confusion matrix for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sb\n",
    "mat = confusion_matrix(digits.target, labels)\n",
    "sb.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=digits.target_names,\n",
    "            yticklabels=digits.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could apply the clustering algorithms to principle components instead of initial 64-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(.80)\n",
    "principalComponents = pca.fit_transform(digits.data)\n",
    "print(\"Number of components:\", principalComponents.shape[1])\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "clusters = kmeans.fit_predict(principalComponents)\n",
    "kmeans.cluster_centers_.shape\n",
    "\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(digits.target[mask])[0]\n",
    "print(\"The accuracy is\", accuracy_score(digits.target, labels))\n",
    "\n",
    "mat = confusion_matrix(digits.target, labels)\n",
    "sb.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=digits.target_names,\n",
    "            yticklabels=digits.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**EM with GMM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_gmm = GaussianMixture(n_components=10)\n",
    "em_gmm_labels=em_gmm.fit_predict(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = em_gmm_labels\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(digits.target[mask])[0]\n",
    "print(\"The accuracy is\", accuracy_score(digits.target, labels))\n",
    "\n",
    "mat = confusion_matrix(digits.target, labels)\n",
    "sb.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=digits.target_names,\n",
    "            yticklabels=digits.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agglomerative clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative = AgglomerativeClustering(n_clusters=10)\n",
    "agglomerative.fit(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = agglomerative.labels_\n",
    "labels = np.zeros_like(clusters)\n",
    "for i in range(10):\n",
    "    mask = (clusters == i)\n",
    "    labels[mask] = mode(digits.target[mask])[0]\n",
    "print(\"The accuracy is\", accuracy_score(digits.target, labels))\n",
    "\n",
    "mat = confusion_matrix(digits.target, labels)\n",
    "sb.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=digits.target_names,\n",
    "            yticklabels=digits.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Consider the beer dataset (beer.xlsx, available at LMS system) which provides the price, the alcohol percentage and calorie content for a series of beer brands. Perform various cluster analysis algorithms to the data and compare the results. What is an appropriate number of clusters? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Consider the cars dataset (Cars.xlsx, available at LMS system) which provides different characteristics for 406 cars:\n",
    "- Car name\n",
    "- MPG (Miles per gallon, a measure of gas mileage)\n",
    "- Number of cylinders\n",
    "- Displacement of the car (in cubic inches)\n",
    "- Horsepower\n",
    "- Weight\n",
    "- Acceleration\n",
    "- Model (Year)\n",
    "- Origin (Nationality of manufacturer)\n",
    "\n",
    "Perform various cluster analysis algorithms to the data and compare them. What is an appropriate number of clusters? Explain your answer. Provide an interpretation of the obtained groups of cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can transform you own distance metric in both exercises and use it as the input for clustering algorithms.\n",
    "\n",
    "The uniqueness, the complexity and originality of the work is encouraged and shall be considered.\n",
    "\n",
    "**Make a report of the work accomplished and send it (the notebook as well as the PDF file) to the LMS system (Projects) by November 7, 2020**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
